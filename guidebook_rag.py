import os
import sys
from operator import itemgetter
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_cohere import CohereRerank
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel



CONFIG = {
    "index_name": "aion2-guide-rag",
    "embedding_model": "text-embedding-3-small",
    "llm_model": "gpt-4o-mini",
    "rerank_model": "rerank-multilingual-v3.0"
}

def get_rag_chain():
    """
    RAG ì²´ì¸ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ ì²´ì¸ì€ invoke ì‹œ {'question': '...', 'chat_history': '...'} í˜•íƒœì˜ ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
    """
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # 2. Retriever & Reranker ì„¤ì •
    embeddings = OpenAIEmbeddings(model=CONFIG["embedding_model"])
    vector_store = PineconeVectorStore.from_existing_index(
        index_name=CONFIG["index_name"],
        embedding=embeddings
    )
    
    base_retriever = vector_store.as_retriever(search_kwargs={"k": 20})
    
    compressor = CohereRerank(
        model=CONFIG["rerank_model"],
        cohere_api_key=os.getenv("COHERE_API_KEY"),
        top_n=3
    )
    
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Chat History ì¶”ê°€)
    # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ 'ì´ì „ ëŒ€í™” ë‚´ìš©' ì„¹ì…˜ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
    template = """
    ë‹¹ì‹ ì€ AION2 ê²Œì„ ê°€ì´ë“œ AIì…ë‹ˆë‹¤.
    ì•„ë˜ì˜ [ì´ì „ ëŒ€í™” ë‚´ìš©]ê³¼ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    1. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
    2. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    
    [ì´ì „ ëŒ€í™” ë‚´ìš©]
    {chat_history}

    [ì°¸ê³  ë¬¸ì„œ]
    {context}

    ì§ˆë¬¸: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=CONFIG["llm_model"], temperature=0)

    # 4. ë¬¸ì„œ í¬ë§·íŒ… í—¬í¼
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # 5. Chain ì¡°ë¦½ (itemgetter ì‚¬ìš©)
    # ì…ë ¥ì´ Dictionaryë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ itemgetterë¡œ ê° ìš”ì†Œë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
    rag_chain = (
        RunnableParallel({
            "context": itemgetter("question") | compression_retriever, # ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰
            "question": itemgetter("question"),                        # ì§ˆë¬¸ ê·¸ëŒ€ë¡œ í†µê³¼
            "chat_history": itemgetter("chat_history")                 # ëŒ€í™” ì´ë ¥ ê·¸ëŒ€ë¡œ í†µê³¼
        })
        .assign(answer=(
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt 
            | model 
            | StrOutputParser()
        ))
    )
    
    return rag_chain

# === Main ===
# === í…ŒìŠ¤íŠ¸ìš© Main í•¨ìˆ˜ ===
if __name__ == "__main__":
    import sys
    
    print("ğŸ§ª [TEST MODE] guidebook_rag.py ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    # 1. ì²´ì¸ ìƒì„±
    try:
        chain = get_rag_chain()
        if chain is None:
            print("âŒ ì²´ì¸ ìƒì„± ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        sys.exit(1)

    # 2. í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
    # ìƒí™©: ì‚¬ìš©ìê°€ ì´ì „ì— 'ê°•í™”'ì— ëŒ€í•´ ë¬¼ì–´ë´¤ê³ , ì´ì–´ì„œ 'ë§ˆì„'ì— ëŒ€í•´ ë¬»ëŠ” ìƒí™©
    test_history = """
    User: ì•„ì´í…œ ê°•í™” ì‹œìŠ¤í…œì— ëŒ€í•´ ì•Œë ¤ì¤˜.
    AI: ì•„ì´í…œ ê°•í™”ëŠ” ê°•í™”ì„, ë§ˆì„, ì¬ì¡°ìœ¨ ì£¼ë¬¸ì„œ ë“±ì„ í†µí•´ ì¥ë¹„ì˜ ëŠ¥ë ¥ì¹˜ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    """
    test_query = "ê·¸ëŸ¼ ë§ˆì„ ê°ì¸ì€ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ê±°ì•¼?"

    print(f"\nğŸ“ [ì…ë ¥ ë°ì´í„°]")
    print(f"   - ì´ì „ ëŒ€í™”: {test_history.strip().replace(chr(10), ' ')}...") # ì¤„ë°”ê¿ˆ ì œê±° í›„ ì¶œë ¥
    print(f"   - í˜„ì¬ ì§ˆë¬¸: {test_query}")
    print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘... (Pinecone ê²€ìƒ‰ + Cohere Rerank + GPT ì¶”ë¡ )")

    # 3. ì²´ì¸ ì‹¤í–‰
    try:
        result = chain.invoke({
            "question": test_query,
            "chat_history": test_history
        })

        # 4. ê²°ê³¼ ì¶œë ¥
        print("-" * 60)
        print(f"ğŸ¤– [AI ë‹µë³€]\n{result['answer']}")
        print("-" * 60)
        
        print("ğŸ“š [ì°¸ê³  ë¬¸ì„œ (Cohere Rerank ê²°ê³¼)]")
        for i, doc in enumerate(result['context']):
            score = doc.metadata.get('relevance_score', 0)
            title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
            # scoreê°€ ë†’ì„ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œì…ë‹ˆë‹¤.
            print(f"   [{i+1}] ì‹ ë¢°ë„: {score:.4f} | ì œëª©: {title}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")