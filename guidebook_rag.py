import os
import json
from operator import itemgetter
from dotenv import load_dotenv

# LangChain Core
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.documents import Document

# Models & Stores
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_cohere import CohereRerank

# Retrievers
from langchain_classic.retrievers import ContextualCompressionRetriever, EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from DebugBM25Retriever import DebugBM25Retriever
from DebugPineconeRetriever import DebugPineconeRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter

CONFIG = {
    "index_name": "aion2-guide-rag",
    "embedding_model": "text-embedding-3-large",
    "llm_model": "gpt-4o-mini",
    "rerank_model": "rerank-multilingual-v3.0",
    "local_data_path": "data/guide_docs.json" # í¬ë¡¤ë§í•œ ë°ì´í„° ê²½ë¡œ
}

def load_bm25_documents():
    """ë¡œì»¬ JSON íŒŒì¼ì„ ì½ì–´ BM25ìš© Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    path = CONFIG["local_data_path"]
    
    if not os.path.exists(path):
        print(f"âš ï¸ ê²½ê³ : '{path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. BM25 ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    print(f"ğŸ“‚ BM25 ì¸ë±ì‹±ì„ ìœ„í•´ '{path}' ë¡œë”© ì¤‘...")
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        # JSON -> Document ê°ì²´ ë³€í™˜
        docs = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in data]
        
        # BM25ë„ ì²­í¬ ë‹¨ìœ„ë¡œ ê²€ìƒ‰í•´ì•¼ ì •í™•í•˜ë¯€ë¡œ ë¶„í•  ìˆ˜í–‰
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = text_splitter.split_documents(docs)
        
        print(f"âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ì´ {len(split_docs)}ê°œ ì²­í¬)")
        return split_docs
        
    except Exception as e:
        print(f"âŒ BM25 ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return []

def get_rag_chain():
    """
    Hybrid Search (Pinecone + BM25) -> Rerank -> LLM ì²´ì¸ ìƒì„±
    """
    load_dotenv()

    # 1. Pinecone Retriever ì„¤ì • (Vector Search)
    embeddings = OpenAIEmbeddings(model=CONFIG["embedding_model"])
    vector_store = PineconeVectorStore.from_existing_index(
        index_name=CONFIG["index_name"],
        embedding=embeddings
    )
    # Rerankerì—ê²Œ ë³´ë‚¼ í›„ë³´êµ° (Vector)
    # pinecone_retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    pinecone_retriever = DebugPineconeRetriever(
        vectorstore=vector_store, 
        search_kwargs={"k": 5}
    )
    
    # 2. BM25 Retriever ì„¤ì • (Keyword Search) [ì¶”ê°€ë¨]
    bm25_docs = load_bm25_documents()

    base_retriever = pinecone_retriever # ê¸°ë³¸ê°’ì€ Pinecone ë‹¨ë…
    
    if bm25_docs:
        bm25_retriever = DebugBM25Retriever.from_documents(bm25_docs)
        bm25_retriever.k = 5 # Rerankerì—ê²Œ ë³´ë‚¼ í›„ë³´êµ° (Keyword)

        # 3. Ensemble (Hybrid) ì„¤ì • [ì¶”ê°€ë¨]
        # weights=[0.5, 0.5]: ë²¡í„°ì™€ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜ë°˜ì”© ë°˜ì˜
        print("ğŸ”— Hybrid Search(Pinecone + BM25) ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        base_retriever = EnsembleRetriever(
            retrievers=[pinecone_retriever, bm25_retriever],
            weights=[0.5, 0.5]
        )
    else:
        print("âš ï¸ Hybrid Search ì‹¤íŒ¨ -> Pinecone ë‹¨ë… ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    # 4. Cohere Rerank ì„¤ì • (ì¬ì •ë ¬)
    # compressor = CohereRerank(
    #     model=CONFIG["rerank_model"],
    #     cohere_api_key=os.getenv("COHERE_API_KEY"),
    #     top_n=5 # ìµœì¢…ì ìœ¼ë¡œ LLMì—ê²Œ ì¤„ 3ê°œë§Œ ì„ ë³„
    # )
    
    # Hybrid Retrieverì˜ ê²°ê³¼ë¥¼ Cohereê°€ ì¬ì •ë ¬
    # compression_retriever = ContextualCompressionRetriever(
    #     base_compressor=compressor,
    #     base_retriever=base_retriever
    # )

    # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    template = """
    ë‹¹ì‹ ì€ AION2 ê²Œì„ ê°€ì´ë“œ AIì…ë‹ˆë‹¤.
    ì•„ë˜ì˜ [ì´ì „ ëŒ€í™” ë‚´ìš©]ê³¼ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    1. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
    2. ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
    3. ì•„ì´í…œ, ìŠ¤í‚¬ ëª…ì¹­ì€ ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”.
    4. ì‚¬ìš©ìê°€ íŠ¹ì • ì§ì—…(ì˜ˆ: ìˆ˜í˜¸ì„±, í˜¸ë²•ì„± ë“±)ì— ëŒ€í•´ ë¬¼ì—ˆë‹¤ë©´, ë°˜ë“œì‹œ í•´ë‹¹ ì§ì—…ì˜ ë¬¸ì„œë§Œ ì°¸ì¡°í•˜ì„¸ìš”.
    5. ë¬¸ì„œì˜ [metadata]ë‚˜ ì œëª©ì„ í™•ì¸í•˜ì—¬ ì§ˆë¬¸í•œ ì§ì—…ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    [ì´ì „ ëŒ€í™” ë‚´ìš©]
    {chat_history}

    [ì°¸ê³  ë¬¸ì„œ]
    {context}

    ì§ˆë¬¸: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=CONFIG["llm_model"], temperature=0)

    # 6. ë¬¸ì„œ í¬ë§·íŒ… í—¬í¼
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # 7. Chain ì¡°ë¦½
    rag_chain = (
        RunnableParallel({
            "context": itemgetter("question") | base_retriever, 
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history") 
        })
        .assign(answer=(
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt 
            | model 
            | StrOutputParser()
        ))
    )
    
    return rag_chain

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ (ìƒëµ ê°€ëŠ¥)
if __name__ == "__main__":
    import sys
    
    print("ğŸ§ª [TEST MODE] guidebook_rag.py ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    # 1. ì²´ì¸ ìƒì„±
    try:
        chain = get_rag_chain()
        if chain is None:
            print("âŒ ì²´ì¸ ìƒì„± ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            sys.exit(1)
        print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        sys.exit(1)

    # 2. í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
    # ìƒí™©: ì‚¬ìš©ìê°€ ì´ì „ì— 'ê°•í™”'ì— ëŒ€í•´ ë¬¼ì–´ë´¤ê³ , ì´ì–´ì„œ 'ë§ˆì„'ì— ëŒ€í•´ ë¬»ëŠ” ìƒí™©
    test_history = ""
    test_query = "ì†ì‚¬ëŠ” ì–´ëŠ í´ë˜ìŠ¤ì˜ ìŠ¤í‚¬ì´ì•¼?"

    print(f"\nğŸ“ [ì…ë ¥ ë°ì´í„°]")
    print(f"   - ì´ì „ ëŒ€í™”: {test_history.strip().replace(chr(10), ' ')}...") # ì¤„ë°”ê¿ˆ ì œê±° í›„ ì¶œë ¥
    print(f"   - í˜„ì¬ ì§ˆë¬¸: {test_query}")
    print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘... (Pinecone ê²€ìƒ‰ + BM25 + GPT ì¶”ë¡ )")

    # 3. ì²´ì¸ ì‹¤í–‰
    try:
        result = chain.invoke({
            "question": test_query,
            "chat_history": test_history
        })

        # 4. ê²°ê³¼ ì¶œë ¥
        print("-" * 60)
        print(f"ğŸ¤– [AI ë‹µë³€]\n{result['answer']}")
        print("-" * 60)
        
        # print("ğŸ“š [ì°¸ê³  ë¬¸ì„œ (Cohere Rerank ê²°ê³¼)]")
        # for i, doc in enumerate(result['context']):
        #     score = doc.metadata.get('relevance_score', 0)
        #     title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
        #     # print(doc.page_content)
        #     # scoreê°€ ë†’ì„ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œì…ë‹ˆë‹¤.
        #     print(f"   [{i+1}] ì‹ ë¢°ë„: {score:.4f} | ì œëª©: {title}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")