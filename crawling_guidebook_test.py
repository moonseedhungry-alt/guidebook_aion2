import os
import time
import sys
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# LangChain ê´€ë ¨
from langchain_core.documents import Document  # Document ê°ì²´ ì§ì ‘ ìƒì„±
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

load_dotenv()

INDEX_NAME = "aion2-guide-rag"

# API Key ê²€ì¦
if not os.getenv("OPENAI_API_KEY") or not os.getenv("PINECONE_API_KEY"):
    print("âŒ Error: .env íŒŒì¼ í™•ì¸ í•„ìš”")
    sys.exit(1)

def collect_nc_guide_urls(start_id, end_id):
    print(f"ğŸš€ ìˆ˜ì§‘ ì‹œì‘: CategoryId {start_id} ~ {end_id}")

    options = webdriver.ChromeOptions()
    # options.add_argument('--headless') # ì˜ ë˜ëŠ”ê±° í™•ì¸í•˜ì‹œë©´ ì£¼ì„ í•´ì œí•˜ì„¸ìš” (ì†ë„ í–¥ìƒ)
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    all_urls = set()

    try:
        for cat_id in range(start_id, end_id + 1):
            url = f"https://aion2.plaync.com/ko-kr/guidebook/list#categoryId={cat_id}"
            print(f"\nğŸ”„ [Category {cat_id}] ì´ë™ ì¤‘...")
            
            driver.get(url)

            try:
                # 1. ëª…í™•í•œ í´ë˜ìŠ¤ëª…ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                wait = WebDriverWait(driver, 10)
                target_class = "ncgbg-guide-depth-2-guide-item-link"
                
                wait.until(EC.presence_of_element_located((By.CLASS_NAME, target_class)))
                time.sleep(1) # ë Œë”ë§ ì•ˆì •í™”

                # 2. í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  ìš”ì†Œë¥¼ ì°¾ìŒ
                elements = driver.find_elements(By.CLASS_NAME, target_class)
                
                count = 0
                for el in elements:
                    # Seleniumì´ hrefë¥¼ ê°€ì ¸ì˜¬ ë•Œ ìë™ìœ¼ë¡œ http://... ì „ì²´ ê²½ë¡œë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
                    full_url = el.get_attribute("href")
                    
                    # ê°€ë” ë¹ˆ ë§í¬ë‚˜ ì¤‘ë³µì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•„í„°ë§
                    if full_url and "view?title=" in full_url:
                        all_urls.add(full_url)
                        count += 1
                
                print(f"   âœ… {count}ê°œ ìˆ˜ì§‘ ì„±ê³µ")

            except Exception:
                print(f"   âš ï¸ [Category {cat_id}] í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ê¸€ì´ ì—†ê±°ë‚˜ ë¡œë”© ì‹¤íŒ¨")
                continue

    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    
    finally:
        driver.quit()

    return list(all_urls) 

def process_and_save_docs(urls):
    """ Seleniumìœ¼ë¡œ ìƒì„¸ í˜ì´ì§€ ë¡œë”© -> Title, Desc, Body íŒŒì‹± -> Pinecone ì €ì¥ """
    if not urls:
        print("âš ï¸ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\n1. ë°ì´í„° ë¡œë”© ë° êµ¬ì¡°í™” ì¤‘... (ëŒ€ìƒ: {len(urls)}ê°œ)")
    
    # ... (Selenium ì„¤ì • ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
    options = webdriver.ChromeOptions()
    options.add_argument('--headless') 
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 ...") # ê¸°ì¡´ User-Agent ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    valid_docs = []

    try:
        for url in urls:
            print(f"   â¡ï¸ ì ‘ì†: {url}")
            driver.get(url)
            
            try:
                wait = WebDriverWait(driver, 10)
                # ë³¸ë¬¸(article)ì´ ëœ° ë•Œê¹Œì§€ ëŒ€ê¸°
                wait.until(EC.presence_of_element_located((By.CLASS_NAME, "ncgbt-article")))
                time.sleep(1) 
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                
                # 1. ì œëª© ìˆ˜ì§‘ (ncgbt-cover-title)
                title_tag = soup.select_one(".ncgbt-cover-title")
                title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

                # 2. ì„¤ëª… ìˆ˜ì§‘ (ncgbt-cover-desc)
                desc_tag = soup.select_one(".ncgbt-cover-desc")
                desc = desc_tag.get_text(strip=True) if desc_tag else "ì„¤ëª… ì—†ìŒ"

                # 3. ë³¸ë¬¸ ìˆ˜ì§‘ (ncgbt-article)
                # ì—¬ëŸ¬ ê°œì˜ articleì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•©ì¹¨
                articles = soup.select(".ncgbt-article")
                body_text = []
                for article in articles:
                    text = article.get_text(separator="\n", strip=True)
                    if text:
                        body_text.append(text)
                
                full_body = "\n\n".join(body_text)
                
                if full_body:
                    # [í•µì‹¬ ì „ëµ] ê²€ìƒ‰ ì •í™•ë„ë¥¼ ìœ„í•´ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ì— ì œëª©ê³¼ ì„¤ëª…ì„ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
                    # ì´ë ‡ê²Œ í•˜ë©´ AIê°€ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤.
                    enriched_content = f"ë¬¸ì„œ ì œëª©: {title}\në¬¸ì„œ ìš”ì•½: {desc}\n\në‚´ìš©:\n{full_body}"

                    # ë©”íƒ€ë°ì´í„° êµ¬ì„± (Pineconeì— ì €ì¥ë  ë¶€ê°€ ì •ë³´)
                    metadata = {
                        "source": url,
                        "title": title,       # ë‚˜ì¤‘ì— ë‹µë³€ ì¶œì²˜ í‘œì‹œì— ì‚¬ìš©
                        "description": desc   # ë‚˜ì¤‘ì— ë‹µë³€ ë³´ì¶© ì„¤ëª…ì— ì‚¬ìš©
                    }

                    doc = Document(
                        page_content=enriched_content, # ì‹¤ì œ ë²¡í„°í™”ë˜ì–´ ê²€ìƒ‰ë˜ëŠ” ë‚´ìš©
                        metadata=metadata              # ê²€ìƒ‰ í›„ ë”°ë¼ì˜¤ëŠ” ê¼¬ë¦¬í‘œ ì •ë³´
                    )
                    
                    valid_docs.append(doc)
                    print(f"      âœ… ìˆ˜ì§‘ ì„±ê³µ: [{title}]")
                else:
                    print("      âš ï¸ ë³¸ë¬¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
                    
            except Exception as e:
                print(f"      âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}")
                continue

    except Exception as e:
        print(f"âŒ ë¸Œë¼ìš°ì € ì—ëŸ¬: {e}")
    finally:
        driver.quit()

    # ... (ì´í›„ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
    # ë‹¤ë§Œ, enriched_content ê¸¸ì´ê°€ ê¸¸ì–´ì¡Œìœ¼ë¯€ë¡œ chunk_size ì¡°ì ˆì„ ê³ ë ¤í•´ë³¼ ë§Œí•©ë‹ˆë‹¤.
    if not valid_docs:
        return

    print("2. í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    splits = text_splitter.split_documents(valid_docs)
    
    # Pinecone ì €ì¥ ë¶€ë¶„ (ê¸°ì¡´ ë™ì¼)
    print(f"3. Pinecone('{INDEX_NAME}')ì— ë°ì´í„° ì €ì¥ ì‹œì‘...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    PineconeVectorStore.from_documents(
        documents=splits,
        embedding=embeddings,
        index_name=INDEX_NAME
    )
    print("ğŸ‰ ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
    # target_urls = [
    #     # "https://aion2.plaync.com/ko-kr/guidebook/view?title=%EA%B2%80%EC%84%B1%20%EC%8A%A4%ED%82%AC"
    #     "https://aion2.plaync.com/ko-kr/guidebook/view?title=%EA%B2%80%EC%84%B1"
    # ]
    target_urls = collect_nc_guide_urls(4234, 4244)

    process_and_save_docs(target_urls)